{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/joshuaokolo/book-to-scrape-web-scraper?scriptVersionId=104052309\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"pip install beautifulsoup4\npip install requests\npip install pandas","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from bs4 import BeautifulSoup as soup\nimport requests \nimport pandas as pd \nimport re\nimport datetime","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the target website's address, i.e., URL\nbooks_url = 'https://books.toscrape.com/index.html'\n# Create a response object to get the web page's HTML content\nget_url = requests.get(books_url)\n# Create a beautiful soup object to parse HTML text with the help of the html.parser\nbooks_soup = soup(get_url.text, 'html.parser')\n# Check website's response\nprint(get_url)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get some intuition by printing out HTML\n# This step is not required to build a web scraper\nprint(get_url.text)\nprint(books_soup)\n# Use prettify() method to make the HTML be nicely formatted\nprint(books_soup.prettify())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_categories():\n# Find all categories\n\ndef fetch_books_by_category():\n# Fetch all the books under a category page by page\n\ndef fetch_current_page_books():\n# Fetch all the books listed on the current page\n# Build a dictionary to store extracted data \n# Append the dictionary to a list\n# Go to next page if current page is not the last one\n\ndef fetch_more_info():\n# Get detailed info about a book\n\ndef fetch_all_books():\n# Fetch all the books of all the categories\n# Return the list of dictionaries that contains all the extracted data\nreturn books_all","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find all the categories listed on the web page\n# This step is used for testing and practicing, which can be skipped for the final scraper\ncategories = books_soup.find('ul', {'class': 'nav nav-list'}).find('li').find('ul').find_all('li')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loop through categories\nfor category in categories: \n# Get category name by extracting the text part of <a> element\n# Strip the spaces before and after the name\ncategory_name = category.find('a').text.strip()\n# Get the URL, which leads to the products list page under the category\ncategory_url_relative = category.find('a').get('href')\n# Complete category's URL by adding the base URL\ncategory_url = base_url_for_category + category_url_relative\nprint(f\"{category_name}'s URL is: {category_url}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify base URL\nbase_url_for_category = 'https://books.toscrape.com/'\nbase_url_for_book = 'https://books.toscrape.com/catalogue'\n# Get the date of scraping\nscraping_date = datetime.date.today()\n# Create a dictionary to convert words to digits\n# We will use it when fetching rating\nw_to_d = {'One': 1,\n'Two': 2,\n'Three': 3,\n'Four': 4,\n'Five': 5\n}\n# Create a list to store all the extracted items\nbooks_all = []","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_all_books(soup):\n    # Fetch all the books information\n    # Return books_all, a list of dictionary that contains all the extracted data\n    \n    # Find all the categories by running find_categories() function\n    categories = find_categories(soup)\n    # Loop through categories\n    for category in categories:\n        # Fetch product by category\n        # Within the fetch_books_by_category function, we will scrape products page by page        \n        category_name = category.find('a').text.strip()\n        fetch_books_by_category(category_name, category)\n        \n    return books_all","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_categories(soup):\n# Find all the categories\n\ncategories = books_soup.find('ul', {'class': 'nav nav-list'}).find('li').find('ul').find_all('li')\n\nreturn categories","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Identify the target website's address\nweb_page_url = 'https...'\n# Create a response object to get the web page's HTML content\nget_url = requests.get(web_page_url)\n# Create a beautiful soup object to parse HTML text with the help of the html.parser\nsoup = BeautifulSoup(get_url.text, 'html.parser')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_books_by_category(category_name, category):\n# Fetch books by category\n# Scrape all the books listed on one page\n# Go to next page if current page is not the last page\n# Break the loop at the last page\n\n# Get category URL, i.e., the link to the first page of books under the category\nbooks_page_url = base_url_for_category + category.find('a').get('href')\n# Scape books page by page only when the next page is available\nwhile True:\n# Retrieve the products list page's HTML\nget_current_page = requests.get(books_page_url)\n# Create a beautiful soup object for the current page\ncurrent_page_soup = soup(get_current_page.text, 'html.parser')\n# Run fetch_current_page_books function to get all the products listed on the current page\nfetch_current_page_books(category_name, current_page_soup)\n# Search for the next page's URL\n# Get the next page's URL if the current page is not the last page\ntry:\nfind_next_page_url = current_page_soup.find('li', {'class':'next'}).find('a').get('href') \n# Find the index of the last '/'\nindex = books_page_url.rfind('/')\n# Skip the string after the last '/' and add the next page url\nbooks_page_url = books_page_url[:index+1].strip() + find_next_page_url \nexcept:\nbreak","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_current_page_books(category_name, current_page_soup):\n# Fetch all the books listed on the current page\n# Build a dictionary to store extracted data \n# Append book information to the books_all list\n\n# Find all products listed on the current page\n# Here, we don’t need to identify the class name of <li> (Do you know why?)\ncurrent_page_books = current_page_soup.find('ol', {'class':'row'}).find_all('li')\n\n# Loop through the products \nfor book in current_page_books: \n# Extract book info of interest\n\n# Get book title\n# Replace get('title') with text to see what will happen\ntitle = book.find('h3').find('a').get('title').strip()\n\n# Get book price\nprice = book.find('p', {'class':'price_color'}).text.strip()\n\n# Get in stock info\ninstock = book.find('p', {'class': 'instock availability'}).text.strip()\n\n# Get rating\n# We will get a list, ['star-rating', 'Two'], by using get('class') only, so here, we slice the list to extract rating only\nrating_in_words = book.find('p').get('class')[1]\nrating = w_to_d[rating_in_words]\n\n# Get link \nlink = book.find('h3').find('a').get('href').strip()\nlink = base_url_for_book + link.replace('../../..', '')\n\n# Get more info about a book by running fetch_more_info function\nproduct_info = fetch_more_info(link)\n\n# Create a book dictionary to store the book’s info\nbook = {\n'scraping_date': scraping_date, \n'book_title': title, \n'category': category_name, \n'price': price,\n'rating': rating,\n'instock': instock,\n# Suppose we’re only interested in availability and UPC only\n'availability': product_info['Availability'],\n'UPC': product_info['UPC'],\n'link':link \n}\n# Append book dictionary to books_all list\nbooks_all.append(book)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fetch_more_info(link):\n# Go to the single product page to get more info \n\n# Get url of the web page\nget_url = requests.get(link)\n# Create a beautiful soup object for the book\nbook_soup = soup(get_url.text, 'html.parser')\n\n# Find the product information table\nbook_table = book_soup.find('table',{'class':'table table-striped'}).find_all('tr')\n# Build a dictionary to store the information in the table\nproduct_info = {}\n# Loop through the table \nfor info in book_table:\n\n# Use header cell as key\nkey = info.find('th').text.strip()\n# Use cell as value\nvalue = info.find('td').text.strip() \nproduct_info[key] = value\n\n# Extract number from availability using Regular Expressions\ntext = product_info['Availability']\n# reassign the number to availability\nproduct_info['Availability'] = re.findall(r'(\\d+)', text)[0]\n\nreturn product_info","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def output(books_list):\n# Convert the list with scraped data to a data frame, drop the duplicates, and save the output as a csv file\n\n# Convert the list to a data frame, drop the duplicates\nbooks_df = pd.DataFrame(books_list).drop_duplicates()\nprint(f'There are totally {len(books_df)} books.')\n# Save the output as a csv file\nbooks_df.to_csv(f'books_scraper_{scraping_date}.csv', index = False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"References:\n\nhttps://books.toscrape.com/index.html\n\nhttps://omdena.com/blog/web-scraping-python-beautifulsoup/\n\nhttps://pypi.org/project/beautifulsoup4/","metadata":{}}]}